{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46c9a4e1-628c-4a6a-a41f-eb3f9901833f",
   "metadata": {},
   "source": [
    "PCLab#5 - Group 2 - Emanuele Sala, Luca Soleri, Fabio Stefana"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8de0ea-51ef-4752-84f4-4a685bc95211",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid #007bff; padding: 10px; background-color: #e9f5ff; border-radius: 5px;\">\n",
    "    <h1 style=\"color: #007bff;\">Importing libraries and Dataset</h1>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63ea0fff-1d3a-453b-8cfb-60cab09f3e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f945631-57da-4f54-a7fb-e7039685746d",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"data/sigwatch_data\"\n",
    "df_list = []\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    if file.endswith(\".dta\"):\n",
    "        file_path = os.path.join(directory, file)\n",
    "        temp_df = pd.read_stata(file_path)\n",
    "        df_list.append(temp_df)\n",
    "        \n",
    "df = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104fb569-7f87-4c05-98bd-40faeda213b6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<div style=\"border: 4px solid #007bff; padding: 10px; background-color: #e9f5ff; border-radius: 5px;\">\n",
    "    <h1 style=\"color: #007bff;\">Preliminary data exploration</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c896bfd-39b3-4d7d-a01d-55f96356c7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With this filter we keep only the banks\n",
    "df = df[df[\"corp_industry_sector1\"] == \"Finance\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670aeac1-6bbd-4c2c-9d4d-8ee1c2558dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And with this filter we only keep the countries form the US, UK or EU\n",
    "countries = ['Austria',\n",
    "             'US',\n",
    "             'Denmark',\n",
    "             'UK',\n",
    "             'Germany',\n",
    "             'Luxembourg',\n",
    "             'France',\n",
    "             'Italy',\n",
    "             'Netherlands',\n",
    "             'Belgium',\n",
    "             'Sweden',\n",
    "             'Spain',\n",
    "             'Ireland',\n",
    "             'Portugal',\n",
    "             'Poland',\n",
    "             'Finland',\n",
    "             'USA',\n",
    "             'Croatia',\n",
    "             'Bulgaria',\n",
    "             'Montenegro',\n",
    "             'Bosnia and Herzegovina']\n",
    "\n",
    "df = df[df['country_corp'].isin(countries)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8f2091-57c9-4064-8a64-95547e1549d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We count the unique number of ud_archive as some have more than one row but still count as one isngle campaing\n",
    "n_of_campaigns = len(list(df[\"uid_archive\"].unique()))\n",
    "print(f\"There are {n_of_campaigns} unique campaigns for US UK and EU banks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5baf485d-3af8-487a-a220-d1d91004a01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_ngo_columns = []\n",
    "for i in range(5):\n",
    "    i = i+1\n",
    "    ngo_column_number = f\"ngo_name{i}\"\n",
    "    ngo_col = list(df[ngo_column_number])\n",
    "    list_of_ngo_columns += ngo_col\n",
    "unique_ngos = list(set(list_of_ngo_columns))\n",
    "\n",
    "# we do -1 because we have to account for the null value\n",
    "print(f\"There are {len(unique_ngos) - 1} unique NGO organizations involved in this dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a84c67-64d6-49a5-bfc7-09587ccfac06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numero di aziende targettate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4d16f5-0ca2-49f6-9512-6079242c6b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_we_want = [\"uid_archive\", \n",
    "                   \"date\", \n",
    "                   \"company\",\n",
    "                   'country_corp', # Country of the Company\n",
    "                   'corp_industry_sector1', # Industry of the company\n",
    "                   'company_parent',\n",
    "                   'company_parent_country',\n",
    "                   \"sentiment\",\n",
    "                   'issue_name1',\n",
    "                   'issue_name2',\n",
    "                   'issue_name3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4daac4-7cea-451a-b66f-e961af076dd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2cb34e4a-836c-4c8b-b106-9547283521ef",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid #007bff; padding: 10px; background-color: #e9f5ff; border-radius: 5px;\">\n",
    "    <h1 style=\"color: #007bff;\">Day 2: Exploring bank data</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163a8bae-b258-4861-8c63-d9713cb0354f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_banks_ri(directory):\n",
    "    # banks_ri.xlsm has the total return for each bank\n",
    "    banks_ri = pd.ExcelFile(directory)\n",
    "    \n",
    "    # Each workbook is divided into sheets that divide the banks into different countires\n",
    "    # the first sheet is a request table so we will ignore it\n",
    "    banks_ri_sheets = banks_ri.sheet_names[1:]\n",
    "    \n",
    "    # We are gonna read all the different sheets and put them into a dataframe, store it \n",
    "    # into a list and concatenate them all toghether so we will have a big df with all the \n",
    "    # banks_ri info in it.\n",
    "    # Since all the sheets have a column for the date, we will only read it for the first\n",
    "    # sheet and skip it for the others.\n",
    "    first_df = banks_ri.parse(banks_ri_sheets[0])\n",
    "    first_df = first_df.rename(columns={first_df.columns[0]: \"Date\"})\n",
    "    other_dfs = [banks_ri.parse(sheet_name).iloc[:, 1:] for sheet_name in banks_ri_sheets[1:]]\n",
    "    \n",
    "    banks_ri_df = pd.concat([first_df] + other_dfs, axis=1)\n",
    "    return banks_ri_df\n",
    "\n",
    "def clean_banks_ri(banks_ri_df):\n",
    "    # Rename columns so that they have cleaner names\n",
    "    rename_dict = {\"Date\": \"Date\"}\n",
    "    for col in banks_ri_df.columns[1:]:\n",
    "        rename_dict[col] = col[:-17] + \"_TR\"\n",
    "    banks_ri_df.rename(columns=rename_dict, inplace=True)\n",
    "    \n",
    "    # As we have 158 banks, we will drop all the banks that are marked as Dead, this is a simple \n",
    "    # solution around this problem and we are only doing this because we can afford to do so \n",
    "    # seeing how many banks we have. \n",
    "    # With this operation we will be dropping only 3 banks.\n",
    "    dead_banks = []\n",
    "    for col in banks_ri_df.columns[1:]:\n",
    "        if \"dead\" in col.lower():\n",
    "            dead_banks.append(col)\n",
    "    banks_ri_df.drop(dead_banks, axis = 1, inplace = True)\n",
    "    return banks_ri_df\n",
    "\n",
    "banks_ri_df = read_banks_ri(\"data/banks_data_bocconi/banks_ri.xlsm\")\n",
    "banks_ri_df = clean_banks_ri(banks_ri_df)\n",
    "banks_ri_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a185186-29c5-46a5-aca9-98fe1155d476",
   "metadata": {},
   "source": [
    "The banks_ri dataset provides the Total Return Index, this includes both price changes and the effect of dividends reinvested back into the stock. If the bank pays dividends, these dividends are considered to be reinvested, adding to the growth of the index. \n",
    "\n",
    "This index provides a more comprehensive view of the stockâ€™s overall return by including income from dividends, making it useful for capturing the full picture of what an investor earns from holding the stock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37534eb4-655e-4758-8ea2-d66504fd6f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_banks_pi(directory):\n",
    "    banks_pi = pd.ExcelFile(directory)\n",
    "    banks_pi_sheets = banks_pi.sheet_names[1:]\n",
    "    first_df = banks_pi.parse(banks_pi_sheets[0])\n",
    "    first_df = first_df.rename(columns={first_df.columns[0]: \"Date\"})\n",
    "    other_dfs = [banks_pi.parse(sheet_name).iloc[:, 1:] for sheet_name in banks_pi_sheets[1:]]\n",
    "    banks_ri_df = pd.concat([first_df] + other_dfs, axis=1)\n",
    "    return banks_ri_df\n",
    "\n",
    "def clean_banks_pi(banks_pi_df):\n",
    "    rename_dict = {\"Date\": \"Date\"}\n",
    "    for col in banks_pi_df.columns[1:]:\n",
    "        rename_dict[col] = col[:-14] + \"_PI\"\n",
    "    banks_pi_df.rename(columns=rename_dict, inplace=True)\n",
    "    dead_banks = []\n",
    "    for col in banks_pi_df.columns[1:]:\n",
    "        if \"dead\" in col.lower():\n",
    "            dead_banks.append(col)\n",
    "    banks_pi_df.drop(dead_banks, axis = 1, inplace = True)\n",
    "    return banks_pi_df\n",
    "\n",
    "banks_pi_df = read_banks_pi(\"data/banks_data_bocconi/banks_pi.xlsm\")\n",
    "banks_pi_df = clean_banks_pi(banks_pi_df)\n",
    "banks_pi_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d380c3a3-46d4-4605-b6fd-5856557895f0",
   "metadata": {},
   "source": [
    "NOTE: The banks_pi dataset contains 3 more banks than the banks_pi dataset, those banks are all 3 located in Austria and they are:\n",
    "- ERSTE GROUP BANK\n",
    "- RAIFFEISEN BANK INTL\n",
    "- VOLKSBANK VBG.PARTN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59af8973-1df9-48ec-ad1c-1be428042adf",
   "metadata": {},
   "source": [
    "On the other hand, the banks_pi dataset contains the Price Index, which only reflects a stockâ€™s price movements without accounting for dividends or other distributions. It provides a pure measure of price appreciation, capturing changes in the stock's market price alone. \n",
    "\n",
    "This makes it more suitable for CAPM estimation, as we are focused on price shifts rather than the total return an investor would earn if they held the stock and reinvested dividends.\n",
    "\n",
    "<strong>To clarify, we'll now rely exclusively on the banks_pi dataset to carry out our analysis.</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffafdfb-6dde-4894-ab49-e6e2a2aa8f42",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid #007bff; padding: 10px\">\n",
    "    <h3 style=\"color: #007bff;\">Now we load the Fama-French info</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d29dae4-31bf-4132-8fd5-e5d7d3eefce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we get the start and end date of our bank data\n",
    "start = banks_pi_df[\"Date\"].iloc[0]\n",
    "end = banks_pi_df[\"Date\"].iloc[-1]\n",
    "\n",
    "# And now we can read in the Factors and keep only the days we need\n",
    "EU_FF = pd.read_excel(\"data/banks_data_bocconi/Europe_3_Factors_Daily.xlsx\")\n",
    "EU_FF['date'] = pd.to_datetime(EU_FF['date'], format=\"%m/%d/%Y\")\n",
    "EU_FF = EU_FF[(EU_FF[\"date\"] >= start) & (EU_FF[\"date\"] <= end)]\n",
    "rename_dict_EU = {\"date\": \"date\"}\n",
    "for col in EU_FF.columns[1:]:\n",
    "    rename_dict_EU[col] = col + \"_EU\"\n",
    "EU_FF.rename(columns=rename_dict_EU, inplace=True)\n",
    "\n",
    "US_FF = pd.read_excel(\"data/banks_data_bocconi/North_America_3_Factors_Daily.xlsx\")\n",
    "US_FF['date'] = pd.to_datetime(US_FF['date'], format=\"%m/%d/%Y\")\n",
    "US_FF = US_FF[(US_FF[\"date\"] >= start) & (US_FF[\"date\"] <= end)]\n",
    "US_FF.drop(\"date\", axis = 1, inplace = True)\n",
    "rename_dict_US = {}\n",
    "for col in US_FF.columns:\n",
    "    rename_dict_US[col] = col + \"_US\"\n",
    "US_FF.rename(columns=rename_dict_US, inplace=True)\n",
    "\n",
    "FF = pd.concat([EU_FF, US_FF], axis = 1)\n",
    "FF.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# Now we calculate the market return, as we have (rm -rf) and rf its quite simple\n",
    "FF[\"MKT_EU\"] = FF[\"Mkt-RF_EU\"] - FF[\"RF_EU\"]\n",
    "FF[\"MKT_US\"] = FF[\"Mkt-RF_US\"] - FF[\"RF_US\"]\n",
    "\n",
    "print(f\"Banks_pi length: {len(banks_pi_df)}\")\n",
    "print(f\"Banks_pi length: {len(FF)}\")\n",
    "FF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc46077-edf1-46a7-9429-08c56ec2b0bf",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid #007bff; padding: 10px\">\n",
    "    <h3 style=\"color: #007bff;\">Now we calculate the daily returns for each bank</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b70c51-b79f-4fc6-adf9-fd7997060865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_return(stock, returns_df):\n",
    "    shifted = returns_df[stock].shift(1)\n",
    "    return_series = (returns_df[stock] - shifted) / shifted\n",
    "    return return_series.rename(f\"{stock}_r\")\n",
    "\n",
    "returns_dict = {}\n",
    "\n",
    "tickers = banks_pi_df.columns[1:]\n",
    "for stock in tickers:\n",
    "    returns_dict[f\"{stock}_r\"] = calculate_return(stock, banks_pi_df)\n",
    "\n",
    "returns_df = pd.concat(returns_dict.values(), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521bf002-1e4c-45e2-ae5c-5f5a2d0532a2",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid #007bff; padding: 10px\">\n",
    "    <h3 style=\"color: #007bff;\">Now we put everything togheter into a single panel</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43932ec-057c-4c82-9b6a-22fc0ed9c76b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bank_data = pd.concat([banks_pi_df, returns_df, FF], axis=1)\n",
    "bank_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c0296b-67bd-40cf-9efa-23b4ae1d5385",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(bank_data[\"Date\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0b22e1-936c-4779-b5a3-1719a46fbfd3",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid #007bff; padding: 10px; background-color: #e9f5ff; border-radius: 5px;\">\n",
    "    <h1 style=\"color: #007bff;\">Task 4: Estimating the CAPMÂ </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31219fb3-fa79-49ce-954e-54db386f61d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "us_banks = []\n",
    "banks_pi = pd.ExcelFile(\"data/banks_data_bocconi/banks_ri.xlsm\")\n",
    "us_banks_raw = list(banks_pi.parse(\"US\").columns)[1:]\n",
    "for bank in us_banks_raw:\n",
    "    us_banks.append(bank[:-17] + \"_PI\")\n",
    "\n",
    "# Delete banks_pi and us_banks_raw to free memory\n",
    "del banks_pi, us_banks_raw\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd06273-eafd-4f7c-95e8-8601f6b8d78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "us_banks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5ffd00-ff8a-4300-809a-2fd31eade034",
   "metadata": {},
   "source": [
    "Now we will estimate the CAPM on a rolling window we decided to use 130 days (as there are about 130 working days in 6 months)as a window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48ebdaf-89c6-4a6e-a633-f779f6b5f6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_mkt_data(bank_data, bank, us_banks):\n",
    "    if bank in us_banks:\n",
    "        mkt_data = bank_data[\"MKT_US\"].copy()\n",
    "    else:\n",
    "        mkt_data = bank_data[\"MKT_EU\"].copy()\n",
    "    return mkt_data\n",
    "\n",
    "def rollin_window_CAPM(bank_data, bank, us_banks, window_size = 150):\n",
    "    current_bank = bank_data[[\"Date\", f\"{bank}\"]].copy()\n",
    "    mkt_data = get_mkt_data(bank_data, bank, us_banks)\n",
    "    current_bank[\"MKT\"] = mkt_data\n",
    "\n",
    "    # Iterate using sliding window approach\n",
    "    for i in range(len(current_bank) - window_size + 1):\n",
    "        window = current_bank.iloc[i:i + window_size] \n",
    "\n",
    "        # Add a constant term (intercept) to the regression model, this will be our Alpha\n",
    "        X = sm.add_constant(window[\"MKT\"])  \n",
    "        # Fit the OLS regression\n",
    "        model = sm.OLS(current_bank, X).fit()  \n",
    "    \n",
    "        # Extract parameter of intrest\n",
    "        alpha = model.params[0]\n",
    "        beta = model.params[1]\n",
    "        p_value_alpha = model.pvalues[0]\n",
    "        p_value_beta = model.pvalues[1]\n",
    "\n",
    "        ## BISOGNA CAPIRE COME MEETTERE VIA I RISULTATI\n",
    "    \n",
    "        # Create a dictionary to store the results for the current stock\n",
    "        result_i = {\"Alpha\": alpha,\n",
    "                    \"Beta\": beta,\n",
    "                    \"P Value Alpha\": p_value_alpha,\n",
    "                    \"P Value Beta\": p_value_beta}\n",
    "        \n",
    "    \n",
    "    \n",
    "    return current_bank\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01d79d32-776b-41bf-8ab0-b9c1975c4c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Date  Value\n",
      "0 2024-01-01      1\n",
      "1 2024-01-02      2\n",
      "2 2024-01-03      3\n",
      "3 2024-01-04      4\n",
      "4 2024-01-05      5\n",
      "        Date  Value\n",
      "1 2024-01-02      2\n",
      "2 2024-01-03      3\n",
      "3 2024-01-04      4\n",
      "4 2024-01-05      5\n",
      "5 2024-01-06      6\n",
      "        Date  Value\n",
      "2 2024-01-03      3\n",
      "3 2024-01-04      4\n",
      "4 2024-01-05      5\n",
      "5 2024-01-06      6\n",
      "6 2024-01-07      7\n",
      "        Date  Value\n",
      "3 2024-01-04      4\n",
      "4 2024-01-05      5\n",
      "5 2024-01-06      6\n",
      "6 2024-01-07      7\n",
      "7 2024-01-08      8\n",
      "        Date  Value\n",
      "4 2024-01-05      5\n",
      "5 2024-01-06      6\n",
      "6 2024-01-07      7\n",
      "7 2024-01-08      8\n",
      "8 2024-01-09      9\n",
      "        Date  Value\n",
      "5 2024-01-06      6\n",
      "6 2024-01-07      7\n",
      "7 2024-01-08      8\n",
      "8 2024-01-09      9\n",
      "9 2024-01-10     10\n"
     ]
    }
   ],
   "source": [
    "# Sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Date': pd.date_range('2024-01-01', periods=10, freq='D'),\n",
    "    'Value': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "})\n",
    "\n",
    "# Window size\n",
    "window_size = 5\n",
    "\n",
    "# Iterate using sliding window approach\n",
    "for i in range(len(df) - window_size + 1):\n",
    "    window = df.iloc[i:i + window_size]  # Select a window of size 'window_size'\n",
    "    print(window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a294cd-4c2b-479c-8563-a0cddda69b32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a55c39d-049f-4c7a-9159-0a84e7185444",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7bd9f9-6c01-4e87-b79b-08de4a6214ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247d1cb6-619f-4a5f-870e-9408086b8002",
   "metadata": {},
   "outputs": [],
   "source": [
    "prova = rollin_window_CAPM(bank_data, \"BANK OF AMERICA_PI\", us_banks)\n",
    "prova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171d33d8-cec4-4ea3-8843-fa722c9c6cc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2598f8-9f60-463b-83d0-a4de558dc23d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bb520c-9b38-4afa-a954-4743fd04379e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29772877-6bc1-41b2-b9bf-c122171fe395",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bb8e94-4b82-4e77-acea-36830915621f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e35e63-cd9f-4bdf-9192-76b584a25a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def estimateCAPM(returns_df):\n",
    "    # Initialize an empty DataFrame to store results\n",
    "    CAPM_df = pd.DataFrame()  \n",
    "     # Loop through all stock columns, excluding the Date and sp500\n",
    "    for stock in list(returns_df.columns)[1:-1]: \n",
    "        stock_i = returns_df[f\"{stock}\"]  \n",
    "        mkt = returns_df[\"sp500_r\"]\n",
    "        \n",
    "        # Add a constant term (intercept) to the regression model, this will be our Alpha\n",
    "        X = sm.add_constant(mkt)  \n",
    "        # Fit the OLS regression\n",
    "        model = sm.OLS(stock_i, X).fit()  \n",
    "    \n",
    "        # Extract parameter of intrest\n",
    "        alpha = model.params[0]\n",
    "        beta = model.params[1]\n",
    "        p_value_alpha = model.pvalues[0]\n",
    "        p_value_beta = model.pvalues[1]\n",
    "    \n",
    "        # Create a dictionary to store the results for the current stock\n",
    "        result_i = {\"Alpha\": alpha,\n",
    "                    \"Beta\": beta,\n",
    "                    \"P Value Alpha\": p_value_alpha,\n",
    "                    \"P Value Beta\": p_value_beta}\n",
    "    \n",
    "        # Convert result_i to a DataFrame and concatenate it to CAPM_df\n",
    "        result_df = pd.DataFrame(result_i, index=[stock])  \n",
    "        CAPM_df = pd.concat([CAPM_df, result_df])  \n",
    "\n",
    "CAPM_df.reset_index(inplace = True)\n",
    "CAPM_df.rename(columns={'index': 'Stock'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2277a3-b1bc-4445-9315-2e4df8fb29ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = int(0.7 * len(banks_ri_df))\n",
    "\n",
    "# Drop columns that don't meet the threshold\n",
    "banks_ri_df.dropna(axis=1, thresh=threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e6aac6-658e-4566-ad66-e1de113f01e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
