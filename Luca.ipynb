{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46c9a4e1-628c-4a6a-a41f-eb3f9901833f",
   "metadata": {},
   "source": [
    "PCLab#5 - Group 2 - Emanuele Sala, Luca Soleri, Fabio Stefana"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8de0ea-51ef-4752-84f4-4a685bc95211",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid #007bff; padding: 10px; background-color: #e9f5ff; border-radius: 5px;\">\n",
    "    <h1 style=\"color: #007bff;\">Importing libraries and Dataset</h1>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63ea0fff-1d3a-453b-8cfb-60cab09f3e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f945631-57da-4f54-a7fb-e7039685746d",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"data/sigwatch_data\"\n",
    "df_list = []\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    if file.endswith(\".dta\"):\n",
    "        file_path = os.path.join(directory, file)\n",
    "        temp_df = pd.read_stata(file_path)\n",
    "        df_list.append(temp_df)\n",
    "        \n",
    "df = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104fb569-7f87-4c05-98bd-40faeda213b6",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px solid #007bff; padding: 10px; background-color: #e9f5ff; border-radius: 5px;\">\n",
    "    <h1 style=\"color: #007bff;\">Preliminary data exploration</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c896bfd-39b3-4d7d-a01d-55f96356c7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With this filter we keep only the banks\n",
    "df = df[df[\"corp_industry_sector1\"] == \"Finance\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "670aeac1-6bbd-4c2c-9d4d-8ee1c2558dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And with this filter we only keep the countries form the US, UK or EU\n",
    "countries = ['Austria',\n",
    "             'US',\n",
    "             'Denmark',\n",
    "             'UK',\n",
    "             'Germany',\n",
    "             'Luxembourg',\n",
    "             'France',\n",
    "             'Italy',\n",
    "             'Netherlands',\n",
    "             'Belgium',\n",
    "             'Sweden',\n",
    "             'Spain',\n",
    "             'Ireland',\n",
    "             'Portugal',\n",
    "             'Poland',\n",
    "             'Finland',\n",
    "             'USA',\n",
    "             'Croatia',\n",
    "             'Bulgaria',\n",
    "             'Montenegro',\n",
    "             'Bosnia and Herzegovina']\n",
    "\n",
    "df = df[df['country_corp'].isin(countries)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a8f2091-57c9-4064-8a64-95547e1549d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2370 unique campaigns for US UK and EU banks\n"
     ]
    }
   ],
   "source": [
    "# We count the unique number of ud_archive as some have more than one row but still count as one isngle campaing\n",
    "n_of_campaigns = len(list(df[\"uid_archive\"].unique()))\n",
    "print(f\"There are {n_of_campaigns} unique campaigns for US UK and EU banks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5baf485d-3af8-487a-a220-d1d91004a01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 932 unique NGO organizations involved in this dataset\n"
     ]
    }
   ],
   "source": [
    "list_of_ngo_columns = []\n",
    "for i in range(5):\n",
    "    i = i+1\n",
    "    ngo_column_number = f\"ngo_name{i}\"\n",
    "    ngo_col = list(df[ngo_column_number])\n",
    "    list_of_ngo_columns += ngo_col\n",
    "unique_ngos = list(set(list_of_ngo_columns))\n",
    "\n",
    "# we do -1 because we have to account for the null value\n",
    "print(f\"There are {len(unique_ngos) - 1} unique NGO organizations involved in this dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a84c67-64d6-49a5-bfc7-09587ccfac06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numero di aziende targettate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c4d16f5-0ca2-49f6-9512-6079242c6b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_we_want = [\"uid_archive\", \n",
    "                   \"date\", \n",
    "                   \"company\",\n",
    "                   'country_corp', # Country of the Company\n",
    "                   'corp_industry_sector1', # Industry of the company\n",
    "                   'company_parent',\n",
    "                   'company_parent_country',\n",
    "                   \"sentiment\",\n",
    "                   'issue_name1',\n",
    "                   'issue_name2',\n",
    "                   'issue_name3']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39573386",
   "metadata": {},
   "source": [
    "# Reddit Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e08a0d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        company                                NGO  \\\n",
      "0               JP Morgan Chase                    Mupo Foundation   \n",
      "1     Goldman Sachs Group, Inc.  Institute for Policy Studies U.S.   \n",
      "2     Goldman Sachs Group, Inc.  Institute for Policy Studies U.S.   \n",
      "3  European Investment Bank EIB              CEE Bankwatch Network   \n",
      "4      Lloyds Banking Group plc                           UK Uncut   \n",
      "\n",
      "         date                                              title  \\\n",
      "0  2011-12-13  Where did the $2 billion that JP Morgan Chase ...   \n",
      "1  2011-12-13  I AMA analyst in investment management at a \"b...   \n",
      "2  2011-12-13  Reddit-Exclusive Offer - 25% off Financial Tra...   \n",
      "3  2011-12-09     The next recession could be a doozy, thoughts?   \n",
      "4  2011-12-06  UK finance geniuses, what is your opinion of C...   \n",
      "\n",
      "                                                text   created_utc  \\\n",
      "0  I kinda get this gist of this:\\n\\n[*In Februar...  1.339052e+09   \n",
      "1  Sorry finance reddit on my tardiness, I am cur...  1.297138e+09   \n",
      "2  Always wanted to land your dream job in financ...  1.311613e+09   \n",
      "3  A large number of people took risky loans to s...  1.306781e+09   \n",
      "4  I have an opportunity to work closely with bot...  1.337290e+09   \n",
      "\n",
      "                                                 url subreddit  \n",
      "0  https://www.reddit.com/r/finance/comments/upfc...   finance  \n",
      "1  https://www.reddit.com/r/finance/comments/fha0...   finance  \n",
      "2  https://www.reddit.com/r/finance/comments/izb9...   finance  \n",
      "3  https://www.reddit.com/r/finance/comments/hnne...   finance  \n",
      "4  https://www.reddit.com/r/finance/comments/ts9a...   finance  \n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Reddit API credentials\n",
    "client_id = \"RQswmwIqjV3DmpuBke34QQ\"\n",
    "client_secret = \"EhyqqRfPu3x4EDVnsGTMD12y0xFbBQ\"\n",
    "user_agent = \"finance_scrape (by u/Kashiko_02)\"\n",
    "\n",
    "# Set up the Reddit client\n",
    "reddit = praw.Reddit(\n",
    "    client_id=client_id,\n",
    "    client_secret=client_secret,\n",
    "    user_agent=user_agent\n",
    ")\n",
    "\n",
    "# Sample data from your dataset\n",
    "#campaigns = [df['company'], df['ngo_name1'], df['date']]\n",
    "\n",
    "df_1 = df.copy()\n",
    "columns_to_keep = ['date', 'company', 'ngo_name1', 'ngo_name2', 'ngo_name3', 'ngo_name4', 'ngo_name5']\n",
    "campaign = df_1.reindex(columns=columns_to_keep)\n",
    "campaign = campaign.iloc[:50]\n",
    "\n",
    "\n",
    "# Data collection\n",
    "reddit_data = []\n",
    "\n",
    "for index, campaign in campaign.iterrows():\n",
    "    ngos = [campaign['ngo_name1'], campaign['ngo_name2'], campaign['ngo_name3'], \n",
    "        campaign['ngo_name4'], campaign['ngo_name5']]\n",
    "    \n",
    "    campaign_date = datetime.strptime(campaign[\"date\"], \"%Y-%m-%d\")\n",
    "    campaign_year = campaign_date.year\n",
    "\n",
    "    # Filter out empty NGO names and join them with spaces\n",
    "    ngos = \"; \".join([ngo for ngo in ngos if ngo])\n",
    "    search_query = f\"{campaign['company']} {ngos}\".strip()  \n",
    "    #    print(search_query)\n",
    "    \n",
    "    \n",
    "    search_limit = 10\n",
    "    \n",
    "    # Perform the search\n",
    "    submissions = reddit.subreddit(\"finance\").search(search_query, limit=search_limit)\n",
    "    \n",
    "    \n",
    "    # Convert campaign date to timestamp range\n",
    "    date_before = campaign_date + timedelta(days=365)\n",
    "    date_after = campaign_date - timedelta(days=365)\n",
    "\n",
    "    # Filter and collect data\n",
    "    for submission in submissions:\n",
    "        # Convert Reddit post date\n",
    "        post_date = datetime.utcfromtimestamp(submission.created_utc)\n",
    "        post_date = datetime.strptime(str(post_date), \"%Y-%m-%d %H:%M:%S\")\n",
    "        post_date = post_date.strftime(\"%Y-%m-%d\")\n",
    "        post_date = datetime.strptime(post_date, \"%Y-%m-%d\")\n",
    "        \n",
    "        \n",
    "        #print(date_after,post_date,date_before)\n",
    "        \n",
    "        # Check if post falls within the date range\n",
    "        if date_after <= post_date <= date_before:\n",
    "            reddit_data.append({\n",
    "                \"company\": campaign[\"company\"],\n",
    "                \"NGO\": ngos,\n",
    "                \"date\": campaign[\"date\"],\n",
    "                \"title\": submission.title,\n",
    "                \"text\": submission.selftext,\n",
    "                \"created_utc\": submission.created_utc,\n",
    "                \"url\": submission.url,\n",
    "            #    \"subreddit\": submission.subreddit.display_name\n",
    "            })\n",
    "            \n",
    "\n",
    "# Save to a DataFrame\n",
    "df_reddit = pd.DataFrame(reddit_data)\n",
    "\n",
    "# Display DataFrame (or save it for analysis)\n",
    "print(df_reddit.head())\n",
    "\n",
    "# The next steps would be to clean the text for sentiment analysis and apply a sentiment model.\n",
    "\n",
    "df_reddit.to_csv(\"reddit_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cfe3ac3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reddit.to_csv(\"reddit_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9678e89f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
